# -*- coding: utf-8 -*-
"""NLP_CBOW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16LOPwtPLWUpV7csndXbtadomnwURAa0b

### Libraries
"""

import numpy as np
import keras.backend as K
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Reshape, Lambda
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.text import Tokenizer
import matplotlib.pyplot as plt
import pandas as pd

"""### Import file"""

file_name = 'text.txt'
corpus = open(file_name).readlines()
#print(corpus)

"""### Data preprocessing

"""

corpus = [sentence for sentence in corpus if sentence.count(" ") >= 2]

# Remove punctuation in text and fit tokenizer on entire corpus

tokenizer = Tokenizer(filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n'+"'")
tokenizer.fit_on_texts(corpus)
#print(corpus)

corpus = tokenizer.texts_to_sequences(corpus) # Convert text to sequence of integer values
n_samples = sum(len(s) for s in corpus)       # Total number of words in the corpus
V = len(tokenizer.word_index) + 1             # Total number of unique words in the corpus

#print(n_samples,V)

# Example of how word to integer mapping looks like in the tokenizer
#print(list((tokenizer.word_index.items())))

# Parameters
window_size = 2 
window_size_corpus = 4

# Set numpy seed for reproducible results
np.random.seed(42)

from keras.preprocessing import sequence

# Prepare the data for the CBOW model
def generate_data_cbow(corpus, window_size, V):
    all_in = []
    all_out = []

    # Iterate over all sentences
    for sentence in corpus:
        L = len(sentence)
        for index, word in enumerate(sentence):
            start = index - window_size
            end = index + window_size + 1

            # Empty list which will store the context words
            context_words = []
            for i in range(start, end):
                # Skip the 'same' word
                if i != index:
                    # Add a word as a context word if it is within the window size
                    if 0 <= i < L:
                        context_words.append(sentence[i])
                    else:
                        # Pad with zero if there are no words 
                        context_words.append(0)
            # Append the list with context words
            all_in.append(context_words)

            # Add one-hot encoding of the target word
            all_out.append(to_categorical(word, V))
                 
    return (np.array(all_in), np.array(all_out))

# Create the training data

X_cbow, y_cbow = generate_data_cbow(corpus, window_size, V)
#print('X_cbow = ',X_cbow,'\ny_cbow = ',y_cbow)

"""### CBOW """

# Create the CBOW architecture
dim = 10

#for dim in dims:
cbow = Sequential()

    # Add an Embedding layer
cbow.add(Embedding(input_dim=V, 
                   output_dim=dim,
                   input_length = window_size*2, # Note that we now have 2L words for each input entry
                   embeddings_initializer='glorot_uniform'))

cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim, )))

cbow.add(Dense(V, activation='softmax', kernel_initializer='glorot_uniform'))

cbow.compile(optimizer=keras.optimizers.Adam(),
             loss='categorical_crossentropy',
             metrics=['accuracy'])
    
cbow.summary()
print("")

# Train CBOW model
cbow.fit(X_cbow, y_cbow, batch_size=64, epochs=500, verbose=1)
print("")

"""### PCA"""

# Weights of the model
weights = cbow.get_weights()

# Get the embedding matrix
embedding = weights[0]
#print(embedding)

def add_0():
  words = list((tokenizer.word_index.items()))
  words.insert(0,('unkown',0))
  return words
words = add_0()

from sklearn.decomposition import PCA

pca = PCA(n_components = 2)
final_result = pca.fit_transform(embedding)
fig, ax = plt.subplots(1,1,figsize = (30,30))
plt.scatter(final_result[:,0], final_result[:,1])
for i, word in enumerate(words):
    ax.annotate(word, xy = (final_result[i, 0], final_result[i, 1]))

"""### Output Prediction"""

# Function to get embedding vectors of a particular word

def embed(word, embedding, vocab_size=V, tokenizer=tokenizer):
    int_word = tokenizer.texts_to_sequences([word])[0]
    bin_word = to_categorical(int_word, V)
    return np.dot(bin_word, embedding)
#print(embed('amnesia',embedding))

# Function to check which word a given y_cbow[i] stands for

def know_word(y_cbow):
  for i in range(len(y_cbow)):
    if y_cbow[i] == 1 :
      break
  return words[i][0]  
#print(know_word(y_cbow[0]))

# Output(middle) word prediction

def output_word(input):
  input_tokens = tokenizer.texts_to_sequences(input)
  #print(input_tokens)
  for i in range(len(cbow.predict(input_tokens)[0])):
    if cbow.predict(input_tokens)[0][i] == cbow.predict(input_tokens)[0].max() :
      break
  return words[i][0]

input_words = [input("Enter the neighbouring words (separated by spaces) :")]
print('input words = ',input_words)

print(output_word(input_words))